{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3c8484-67b8-4d1e-95a1-471afee8fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236d9dce-797f-4be1-bbf1-cf4e8b07109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone \n",
    "from pinecone import ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64b4b16-f194-47fd-898a-bef7f28befaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc =Pinecone(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bb6b13-eb13-4aaa-b029-63ac5eee3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ServerlessSpec(cloud='aws',region='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92a5e16-3a53-4284-8aa6-5317cbea71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'keys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e49f070-6fa9-4901-9335-7f6486765bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7074f16d-8ebc-40b0-8c01-d1d8a78591fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 384\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=dimension,\n",
    "    metric = 'cosine',\n",
    "    spec = spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bf22e8-da49-4b7e-81b1-942d1e297ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997fbc8d-f221-4eb9-9492-bd48671f6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import  SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e5114e-737d-45c6-ad0b-9dcd537a98e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f599dd2c-96ff-4c7a-a6e1-3f449c204d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73c268c5-bcbf-44e1-a02f-60a544a02042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Wikipedia , Jupiter is the fifth ...</td>\n",
       "      <td>source: wikipedia , name:jupiter , fifth plane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>According to Wikipedia , Mars is the fourth pl...</td>\n",
       "      <td>source: Wikipedia, name: Mars, fourth planet f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>According to Wikipedia , Greek (Modern Greek: ...</td>\n",
       "      <td>source: Wikipedia, name: Greek, language famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According toWikipedia , Virat Kohli (Hindi pro...</td>\n",
       "      <td>source: Wikipedia, name: Virat Kohli, born: 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Wikipedia , India, officially the...</td>\n",
       "      <td>source: Wikipedia, name: India, official name:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>According to Britannica, Mount Everest, mounta...</td>\n",
       "      <td>source: Britannica, name: Mount Everest, locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>According to WWF , Rivers are unpredictable, a...</td>\n",
       "      <td>source: WWF, region: Amazon River Basin, chara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>According to International Fund for Animal Wel...</td>\n",
       "      <td>Name: Great White Shark, Largest predatory fis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>According to wikipedia, Interstellar is a 2014...</td>\n",
       "      <td>Source: Wikipedia , Name: Interstellar , Relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Serbian-American engineer and physicist Nikola...</td>\n",
       "      <td>source: Wikipedia , name: Nikola Tesla , born:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>According to Wikipedia , Arduino (/ɑːrˈdwiːnoʊ...</td>\n",
       "      <td>source: Wikipedia , name: Arduino , type: open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>According to wikipedia , The Samsung Galaxy S7...</td>\n",
       "      <td>source: Wikipedia , name: Samsung Galaxy S7 se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>According to IBM, Deep learning is a subset of...</td>\n",
       "      <td>source: IBM , name: Deep Learning , type: subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>According to Wikipedia , World War II[b] or th...</td>\n",
       "      <td>source: Historical Overview\\nname: World War I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>According to World Nuclear Association , The a...</td>\n",
       "      <td>source: World Nuclear Association\\nname: Chern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>According to Oxford , A  ‘genomically’ humaniz...</td>\n",
       "      <td>source: Oxford\\ntopic: Genomically Humanized A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>According to Wikipedia , The Cold War was a pe...</td>\n",
       "      <td>source: General historical overview\\ntopic: Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>According to Unesco , These 34 monasteries and...</td>\n",
       "      <td>source: UNESCO\\ntopic: Ellora Caves\\n\\nlocatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>According to Wikipedia , The Chola dynasty[a] ...</td>\n",
       "      <td>source: Wikipedia, name: Chola dynasty, origin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>According to Wikipedia , The Joint Entrance Ex...</td>\n",
       "      <td>source: Wikipedia, name: Joint Entrance Examin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   According to Wikipedia , Jupiter is the fifth ...   \n",
       "1   According to Wikipedia , Mars is the fourth pl...   \n",
       "2   According to Wikipedia , Greek (Modern Greek: ...   \n",
       "3   According toWikipedia , Virat Kohli (Hindi pro...   \n",
       "4   According to Wikipedia , India, officially the...   \n",
       "5   According to Britannica, Mount Everest, mounta...   \n",
       "6   According to WWF , Rivers are unpredictable, a...   \n",
       "7   According to International Fund for Animal Wel...   \n",
       "8   According to wikipedia, Interstellar is a 2014...   \n",
       "9   Serbian-American engineer and physicist Nikola...   \n",
       "10  According to Wikipedia , Arduino (/ɑːrˈdwiːnoʊ...   \n",
       "11  According to wikipedia , The Samsung Galaxy S7...   \n",
       "12  According to IBM, Deep learning is a subset of...   \n",
       "13  According to Wikipedia , World War II[b] or th...   \n",
       "14  According to World Nuclear Association , The a...   \n",
       "15  According to Oxford , A  ‘genomically’ humaniz...   \n",
       "16  According to Wikipedia , The Cold War was a pe...   \n",
       "17  According to Unesco , These 34 monasteries and...   \n",
       "18  According to Wikipedia , The Chola dynasty[a] ...   \n",
       "19  According to Wikipedia , The Joint Entrance Ex...   \n",
       "\n",
       "                                                    1  \n",
       "0   source: wikipedia , name:jupiter , fifth plane...  \n",
       "1   source: Wikipedia, name: Mars, fourth planet f...  \n",
       "2   source: Wikipedia, name: Greek, language famil...  \n",
       "3   source: Wikipedia, name: Virat Kohli, born: 5 ...  \n",
       "4   source: Wikipedia, name: India, official name:...  \n",
       "5   source: Britannica, name: Mount Everest, locat...  \n",
       "6   source: WWF, region: Amazon River Basin, chara...  \n",
       "7   Name: Great White Shark, Largest predatory fis...  \n",
       "8   Source: Wikipedia , Name: Interstellar , Relea...  \n",
       "9   source: Wikipedia , name: Nikola Tesla , born:...  \n",
       "10  source: Wikipedia , name: Arduino , type: open...  \n",
       "11  source: Wikipedia , name: Samsung Galaxy S7 se...  \n",
       "12  source: IBM , name: Deep Learning , type: subs...  \n",
       "13  source: Historical Overview\\nname: World War I...  \n",
       "14  source: World Nuclear Association\\nname: Chern...  \n",
       "15  source: Oxford\\ntopic: Genomically Humanized A...  \n",
       "16  source: General historical overview\\ntopic: Co...  \n",
       "17  source: UNESCO\\ntopic: Ellora Caves\\n\\nlocatio...  \n",
       "18  source: Wikipedia, name: Chola dynasty, origin...  \n",
       "19  source: Wikipedia, name: Joint Entrance Examin...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('RAG.csv',header=None)\n",
    "columns = [i for i in range(2,15) ]\n",
    "df = df.drop(df[columns] , axis = 1)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71df8b55-14f5-4719-b6f9-2a884beeef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list()\n",
    "for i in df[1]:\n",
    "    key_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1557d410-5e00-4084-96cb-e93c9b439b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_text_list = list()\n",
    "for i in df[1]:\n",
    "    Full_text_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5eac199f-6f8f-4513-a92b-55b7cba1d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(key_list)):\n",
    "    embedding = embed_text(key_list[i])\n",
    "    index.upsert(vectors = [(f\"key_{i}\",embedding.tolist() , {\"text\":key_list[i]})])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f32e62a7-aabe-4d3e-8df1-21019298381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_DB(query):\n",
    "    query_embedding = embed_text(query)\n",
    "    result = index.query(\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return result['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3796500-317b-4f9a-b813-4a867f9e3366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " mass of jupiter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'key_0',\n",
       "  'metadata': {'text': 'source: wikipedia , name:jupiter , fifth planet from '\n",
       "                       'sun , largest planet in solar system , mass: 2.5times '\n",
       "                       'of all other planets etc'},\n",
       "  'score': 0.5779819,\n",
       "  'sparse_values': {'indices': [], 'values': []},\n",
       "  'values': []},\n",
       " {'id': 'key_1',\n",
       "  'metadata': {'text': 'source: Wikipedia, name: Mars, fourth planet from the '\n",
       "                       'Sun, nickname: the Red Planet, surface color: '\n",
       "                       'orange-red (due to iron(III) oxide dust), brightness: '\n",
       "                       'among the brightest objects in Earth’s sky, '\n",
       "                       'classification: terrestrial planet, size: second '\n",
       "                       'smallest in the Solar System, diameter: 6,779 km (4,212 '\n",
       "                       'mi), Martian day (sol): 24.5 hours, Martian year: 1.88 '\n",
       "                       'Earth years (687 Earth days), satellites: two (Phobos '\n",
       "                       'and Deimos)'},\n",
       "  'score': 0.24928299,\n",
       "  'sparse_values': {'indices': [], 'values': []},\n",
       "  'values': []},\n",
       " {'id': 'key_17',\n",
       "  'metadata': {'text': 'source: UNESCO\\n'\n",
       "                       'topic: Ellora Caves\\n'\n",
       "                       '\\n'\n",
       "                       'location and description:\\n'\n",
       "                       '\\n'\n",
       "                       'The Ellora Caves are a complex of 34 monasteries and '\n",
       "                       'temples carved into a high basalt cliff near '\n",
       "                       'Aurangabad, Maharashtra, India.\\n'\n",
       "                       'The site extends over more than 2 km and contains '\n",
       "                       'monuments dating from A.D. 600 to 1000.\\n'\n",
       "                       'cultural significance:\\n'\n",
       "                       '\\n'\n",
       "                       'Ellora is recognized as a unique artistic creation and '\n",
       "                       'technological achievement that showcases the spirit of '\n",
       "                       'tolerance in ancient India.\\n'\n",
       "                       'The caves illustrate the coexistence of three prominent '\n",
       "                       'religions: Buddhism, Brahmanism, and Jainism.\\n'\n",
       "                       'historical phases of excavation:\\n'\n",
       "                       '\\n'\n",
       "                       'Buddhist Caves (Caves 1-12):\\n'\n",
       "                       '\\n'\n",
       "                       'Excavated between the 5th and 8th centuries.\\n'\n",
       "                       'Reflect the Mahayana philosophy prevalent in the '\n",
       "                       'region.\\n'\n",
       "                       'Important caves include:\\n'\n",
       "                       'Cave 10 (Visvakarma/Sutar-ki-jhopari): Known as the '\n",
       "                       'Carpenter’s cave.\\n'\n",
       "                       'Cave 11: Another significant Buddhist cave.\\n'\n",
       "                       'Cave 12 (Teen Tal): The largest three-storied monastery '\n",
       "                       'in this category.\\n'\n",
       "                       'Brahmanical Caves (Caves 13-29):\\n'\n",
       "                       '\\n'\n",
       "                       'Excavated between the 7th and 10th centuries.\\n'\n",
       "                       'Notable caves include:\\n'\n",
       "                       'Cave 15 (Dasavatara): Cave of Ten Incarnations.\\n'\n",
       "                       'Cave 16 (Kailasa): The largest monolithic temple, '\n",
       "                       'showcasing structural innovation and elaborate '\n",
       "                       'craftsmanship.\\n'\n",
       "                       'Features include the sculpture of Ravana attempting to '\n",
       "                       'lift Mount Kailasa and beautiful paintings on the '\n",
       "                       'ceilings.\\n'\n",
       "                       'Cave 21 (Ramesvara) and Cave 29 (Dumar Lena): Other '\n",
       "                       'prominent caves in this group.\\n'\n",
       "                       'Jaina Caves (Caves 30-34):\\n'\n",
       "                       '\\n'\n",
       "                       'Excavated between the 9th and 12th centuries.\\n'\n",
       "                       'Characterized by fine, delicate sculptures and '\n",
       "                       'paintings dedicated to the Digambara sect.\\n'\n",
       "                       'artistic and cultural contributions:\\n'\n",
       "                       '\\n'\n",
       "                       'The Ellora Caves provide insights into ancient Indian '\n",
       "                       'socio-cultural phenomena, material culture, politics, '\n",
       "                       'and lifestyles through their art and architecture.\\n'\n",
       "                       'The architectural activities reflect a spirit of '\n",
       "                       'co-existence and religious tolerance among the '\n",
       "                       'followers of Buddhism, Brahmanism, and Jainism.\\n'\n",
       "                       'overall significance:\\n'\n",
       "                       '\\n'\n",
       "                       'The Ellora Caves are a testament to the architectural '\n",
       "                       'ingenuity and artistic excellence of ancient India, '\n",
       "                       'symbolizing the rich cultural heritage and the '\n",
       "                       'harmonious coexistence of different religious '\n",
       "                       'traditions.'},\n",
       "  'score': 0.09703445,\n",
       "  'sparse_values': {'indices': [], 'values': []},\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = input()\n",
    "search_DB(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23bac1db-0f1e-42de-8737-bee5d2959114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a57f38d4-92ba-4214-bb85-680bb4b70340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 8 planets in our solar system, in order from the Sun:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune"
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model='llama3.2:3b',\n",
    "  messages=[{'role': 'user', 'content': 'Name all planets in solar system'}],\n",
    "  stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'],end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a570c3b-e102-4dca-b25c-a3f53802559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(qeury):\n",
    "    context = search_DB(query)\n",
    "    stream = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': f'Based on this content{context} , answer the {query}'}],\n",
    "    stream=True\n",
    "    )\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'],end='',flush=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "120e4d8d-268e-4020-a6c1-b0aa40743205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is Joint Entrance Exam ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Joint Entrance Examination (JEE) is an engineering entrance assessment in India that serves as a gateway to admission into various engineering colleges in India. It consists of two main components: JEE-Main and JEE-Advanced.\n",
      "\n",
      "Here's a brief overview:\n",
      "\n",
      "1. **Purpose:** The primary purpose of the JEE is to assess the academic ability and aptitude of students seeking admission into Indian Institutes of Technology (IITs), National Institutes of Technology (NITs), and other engineering colleges in India.\n",
      "2. **Components:**\n",
      "\t* **JEE-Main:** A paper-based examination that tests a student's understanding of mathematics, physics, and chemistry. It is designed to assess the basic mathematical skills required for undergraduate studies.\n",
      "\t* **JEE-Advanced:** An online examination that tests a student's problem-solving skills in mathematics, physics, and chemistry. It is designed to assess the advanced mathematical skills required for admission into IITs.\n",
      "3. **Admission basis:** Admission to engineering colleges using JEE-Main or JEE-Advanced scores, with priority given to those who score well in JEE-Advanced.\n",
      "4. **Institutes using JEE-Advanced:**\n",
      "\t* Indian Institutes of Technology (IITs)\n",
      "\t* Indian Institute of Science Education and Research (IISERs)\n",
      "\t* Indian Institute of Technology Bombay (IITB) - Department of Electrical Engineering\n",
      "\t* Indian Institute of Technology Guwahati (IITG) - Department of Computer Science and Engineering\n",
      "5. **Retake policy:** The JEE-Advanced has a re-examination policy, where students can retake the examination if they fail to clear it in two attempts.\n",
      "\n",
      "Overall, the Joint Entrance Examination is an important assessment tool for engineering students in India, providing them with an opportunity to secure admission into top engineering institutions."
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "answer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b70f40-a050-4925-a86d-072fedb06ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
